# Stochastic Gradient Descent (SGD) 
It is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) 
Support Vector Machines and Logistic Regression. The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss 
functions and penalties for classification.

## The advantages of Stochastic Gradient Descent are:

1. Efficient  
2. Ease of implementation   

## The disadvantages of Stochastic Gradient Descent include:

1. SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations  
2. SGD is sensitive to feature scaling 
